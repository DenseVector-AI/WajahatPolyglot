{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a746f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\MCS_Project\\even\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "e:\\MCS_Project\\even\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\wajah\\.cache\\huggingface\\hub\\datasets--sahil2801--CodeAlpaca-20k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating train split: 100%|██████████| 20022/20022 [00:00<00:00, 182201.99 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"sahil2801/CodeAlpaca-20k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "363ad386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CodeAlpaca-20k dataset...\n",
      "Dataset loaded successfully!\n",
      "Total entries in dataset: 20022\n",
      "Available columns: ['output', 'instruction', 'input']\n",
      "Taking first 4000 entries...\n",
      "Converting to instruction-input-output format...\n",
      "Created 4000 valid entries\n",
      "\n",
      "==================================================\n",
      "SAMPLING STATISTICS\n",
      "==================================================\n",
      "Original dataset size: 20022\n",
      "First sample size: 4000\n",
      "Valid entries created: 4000\n",
      "==================================================\n",
      "\n",
      "PREVIEW (First 3 entries):\n",
      "------------------------------\n",
      "Entry 1:\n",
      "  Instruction: Create an array of length 5 which contains all even numbers between 1 and 10....\n",
      "  Input: (empty)\n",
      "  Output: arr = [2, 4, 6, 8, 10]...\n",
      "\n",
      "Entry 2:\n",
      "  Instruction: Formulate an equation to calculate the height of a triangle given the angle, side lengths and opposi...\n",
      "  Input: (empty)\n",
      "  Output: Height of triangle = opposite side length * sin (angle) / side length...\n",
      "\n",
      "Entry 3:\n",
      "  Instruction: Write a replace method for a string class which replaces the given string with a given set of charac...\n",
      "  Input: string = \"Hello World!\"\n",
      "replace_with = \"Greetings!...\n",
      "  Output: def replace(self, replace_with):\n",
      "    new_string = \"\"\n",
      "    for char in self:\n",
      "        if char == \" \":\n",
      " ...\n",
      "\n",
      "Writing dataset to: e:\\MCS_Project\\Final10KEach\\codealpaca_4k_first_sample.jsonl\n",
      "✅ Successfully wrote 4000 entries to e:\\MCS_Project\\Final10KEach\\codealpaca_4k_first_sample.jsonl\n",
      "\n",
      "🎉 Complete! Created 4000 coding examples in JSONL format\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "def load_codealpaca_sample(sample_size=4000):\n",
    "    \"\"\"\n",
    "    Load CodeAlpaca dataset and take first 4k entries\n",
    "    \"\"\"\n",
    "    print(\"Loading CodeAlpaca-20k dataset...\")\n",
    "    try:\n",
    "        ds = load_dataset(\"sahil2801/CodeAlpaca-20k\")\n",
    "        print(f\"Dataset loaded successfully!\")\n",
    "        \n",
    "        # Get the train split\n",
    "        train_data = ds['train']\n",
    "        print(f\"Total entries in dataset: {len(train_data)}\")\n",
    "        \n",
    "        # Convert to pandas DataFrame for easier sampling\n",
    "        df = train_data.to_pandas()\n",
    "        print(f\"Available columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Check if we have enough data\n",
    "        if len(df) < sample_size:\n",
    "            print(f\"Warning: Dataset has only {len(df)} entries, taking all available\")\n",
    "            sample_size = len(df)\n",
    "        \n",
    "        # Take first 4k entries\n",
    "        print(f\"Taking first {sample_size} entries...\")\n",
    "        sample_df = df.head(sample_size)\n",
    "        \n",
    "        # Create final dataset with instruction, input, output format\n",
    "        print(\"Converting to instruction-input-output format...\")\n",
    "        final_dataset = []\n",
    "        \n",
    "        for _, row in sample_df.iterrows():\n",
    "            # Handle different possible column names\n",
    "            instruction = row.get('instruction', row.get('prompt', ''))\n",
    "            input_text = row.get('input', row.get('context', ''))\n",
    "            output_text = row.get('output', row.get('response', row.get('completion', '')))\n",
    "            \n",
    "            # Ensure we have instruction and output at minimum\n",
    "            if instruction and output_text:\n",
    "                entry = {\n",
    "                    \"instruction\": instruction,\n",
    "                    \"input\": input_text if input_text else \"\",\n",
    "                    \"output\": output_text\n",
    "                }\n",
    "                final_dataset.append(entry)\n",
    "        \n",
    "        print(f\"Created {len(final_dataset)} valid entries\")\n",
    "        \n",
    "        # Show statistics\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"SAMPLING STATISTICS\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Original dataset size: {len(train_data)}\")\n",
    "        print(f\"First sample size: {sample_size}\")\n",
    "        print(f\"Valid entries created: {len(final_dataset)}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Show preview\n",
    "        print(\"\\nPREVIEW (First 3 entries):\")\n",
    "        print(\"-\"*30)\n",
    "        for i, entry in enumerate(final_dataset[:3], 1):\n",
    "            print(f\"Entry {i}:\")\n",
    "            print(f\"  Instruction: {entry['instruction'][:100]}...\")\n",
    "            print(f\"  Input: {entry['input'][:50]}...\" if entry['input'] else \"  Input: (empty)\")\n",
    "            print(f\"  Output: {entry['output'][:100]}...\")\n",
    "            print()\n",
    "        \n",
    "        return final_dataset\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return []\n",
    "\n",
    "def save_to_jsonl(data, output_path):\n",
    "    \"\"\"\n",
    "    Save data to JSONL file\n",
    "    \"\"\"\n",
    "    print(f\"Writing dataset to: {output_path}\")\n",
    "    try:\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for entry in data:\n",
    "                f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "        print(f\"✅ Successfully wrote {len(data)} entries to {output_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error writing file: {e}\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    # Load first 4k sample\n",
    "    sample_data = load_codealpaca_sample(sample_size=4000)\n",
    "    \n",
    "    if not sample_data:\n",
    "        print(\"Failed to load sample data\")\n",
    "        return\n",
    "    \n",
    "    # Save to JSONL file\n",
    "    output_path = os.path.join(os.getcwd(), \"codealpaca_4k_first_sample.jsonl\")\n",
    "    save_to_jsonl(sample_data, output_path)\n",
    "    \n",
    "    print(f\"\\n🎉 Complete! Created {len(sample_data)} coding examples in JSONL format\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f9dd99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "even",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
